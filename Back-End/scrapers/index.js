/**
 * indexScrapers.js - Point d'entr√©e principal CORRIG√â
 *
 * ‚úÖ CORRECTIONS APPLIQU√âES :
 * 1. R√©sum√© g√©n√©r√© APR√àS que tous les scrapers soient termin√©s
 * 2. Monitoring qui s'adapte automatiquement aux nouvelles mosqu√©es
 * 3. Mapping dynamique au lieu de statique
 */

const { TimeValidator, browserUtils } = require("./utils/scraper");
const scraperQueue = require("./queue");
const { monitoring } = require("./utils/monitoring");
const { createLogger } = require("./utils/logger");
const scrapingEventEmitter = require("./utils/scrapingEventEmitter");

// Logger principal
const logger = createLogger("ScraperManager");

// Lazy loading des scrapers par pays
let countryScrapers = null;
const getCountryScrapers = () => {
  if (!countryScrapers) {
    countryScrapers = require("./countries");
    logger.debug("üîß Chargement lazy des scrapers par pays");
  }
  return countryScrapers;
};

// Configuration automatique des scrapers depuis la structure par pays/villes
class ScraperManager {
  constructor() {
    this.retryAttempts = new Map();
    this.MAX_RETRIES = 3;
    this.scrapers = new Map();
    this.activeScrapings = new Map(); // Pour suivre les scrapings en cours

    // ‚úÖ CORRECTION 2: Mapping dynamique au lieu de statique
    this.setupScrapersWithDynamicMapping();

    logger.info("üöÄ ScraperManager initialis√© avec optimisations avanc√©es");
  }

  // ‚úÖ NOUVELLE M√âTHODE: Configuration dynamique qui s'adapte automatiquement
  setupScrapersWithDynamicMapping() {
    logger.info(
      "===== Configuration DYNAMIQUE du mapping DB ID ‚Üí Scrapers ====="
    );

    // Import des scrapers par structure hi√©rarchique
    const countryScrapersModule = getCountryScrapers();
    const allScrapers = countryScrapersModule.getAllScrapersFlat();

    // ‚úÖ CORRECTION 2: Mapping dynamique bas√© sur la d√©couverte automatique
    // Chaque scraper d√©couvert automatiquement re√ßoit un ID s√©quentiel
    const dynamicMapping = {};
    let currentId = 1;

    // Pour maintenir la compatibilit√© avec l'ancien syst√®me,
    // on peut garder un mapping de base pour les mosqu√©es existantes
    const legacyMapping = {
      // ‚úÖ MOSQU√âES DE WALSALL (5 mosqu√©es - IDs 1, 2, 3, 19, 20)
      aishaMosque: 1,
      masjidAbuBakr: 2, // Walsall
      masjidAlAqsa: 3,
      masjidAlfarouq: 19,
      masjidEUsman: 20,

      // ‚úÖ MOSQU√âES DE BIRMINGHAM (19 mosqu√©es - IDs 4-18, 21-24)
      amanahMasjid: 4,
      arRahmaCentre: 5,
      bournvilleMasjid: 6,
      centralMosque: 7,
      greenLaneMasjid: 8,
      hallGreenMosque: 9,
      jameMasjid: 10,
      sparkbrookMasjid: 11,
      mahmudSabirMasjid: 12,
      masjidAnnoor: 13,
      masjidAsSunnahAnNabawiyyah: 14,
      masjidEHamza: 15,
      masjidEsaIbnMaryama: 16,
      masjidSulayman: 17,
      kingsHeathMosque: 18,
      masjidUmar: 21,
      muslimStudentsHouse: 22,
      qubaIsmalicCenter: 23,
      masjidAbuBakrBham: 24,
    };

    // Mapper d'abord les mosqu√©es connues
    allScrapers.forEach((scraperData) => {
      const legacyId = legacyMapping[scraperData.key];
      if (legacyId) {
        dynamicMapping[legacyId] = scraperData.key;
      }
    });

    // ‚úÖ PUIS ajouter automatiquement les nouvelles mosqu√©es avec des IDs s√©quentiels
    let nextAvailableId =
      Math.max(...Object.keys(legacyMapping).map((k) => legacyMapping[k])) + 1;

    allScrapers.forEach((scraperData) => {
      const isAlreadyMapped = Object.values(dynamicMapping).includes(
        scraperData.key
      );
      if (!isAlreadyMapped) {
        dynamicMapping[nextAvailableId] = scraperData.key;
        logger.info(
          `üÜï Nouvelle mosqu√©e ajout√©e: ID ${nextAvailableId} ‚Üí ${scraperData.name}`
        );
        nextAvailableId++;
      }
    });

    // Configuration automatique avec mapping dynamique
    for (const [dbId, scraperKey] of Object.entries(dynamicMapping)) {
      const scraperData = allScrapers.find((s) => s.key === scraperKey);
      if (scraperData) {
        this.scrapers.set(
          parseInt(dbId),
          this.createRobustScraper(parseInt(dbId), {
            name: scraperData.name,
            fn: scraperData.fn,
          })
        );
      } else {
        logger.error(
          `‚ùå Scraper '${scraperKey}' non trouv√© pour DB ID ${dbId}`
        );
      }
    }

    // ‚úÖ CORRECTION 2: Log dynamique du nombre total
    const totalMosques = this.scrapers.size;
    logger.info(
      `===== ${totalMosques} scrapers configur√©s avec mapping DYNAMIQUE =====`
    );
    logger.info(
      `üìä R√©partition: ${totalMosques} mosqu√©es configur√©es (auto-d√©couvertes)`
    );

    // Log des nouvelles mosqu√©es si il y en a
    if (totalMosques > 24) {
      logger.info(
        `üéâ ${
          totalMosques - 24
        } nouvelle(s) mosqu√©e(s) ajout√©e(s) automatiquement !`
      );
    }
  }

  createRobustScraper(id, config) {
    return async () => {
      // Si d√©j√† en cours, attendre sans log suppl√©mentaire
      if (scraperQueue.isProcessing(id)) {
        return await scraperQueue.getActiveTask(id);
      }

      // √âmettre l'√©v√©nement de d√©marrage
      scrapingEventEmitter.emitScraperStarted(id, config.name);
      logger.scraping.start(config.name, id);

      // Pr√©parer les informations de mosqu√©e pour le fallback
      const mosqueInfo = {
        id: id,
        name: config.name,
        city: this.getCityFromId(id),
        country: "uk",
      };

      try {
        const result = await scraperQueue.enqueue(
          id,
          async () => {
            // Correction: appeler directement la fonction export√©e
            const rawResult = await config.fn();

            if (!rawResult || !rawResult.times) {
              throw new Error(`Format invalide depuis ${config.name}`);
            }

            // Log de succ√®s
            logger.scraping.success(config.name, Date.now() - Date.now());
            return {
              source: config.name,
              date: rawResult.date,
              times: rawResult.times,
              mosqueId: id,
              scrapedAt: new Date().toISOString(),
              city: mosqueInfo.city,
              country: mosqueInfo.country,
            };
          },
          mosqueInfo
        ); // Passer mosqueInfo pour le fallback

        // √âmettre l'√©v√©nement de completion (succ√®s)
        scrapingEventEmitter.emitScraperCompleted(id, config.name, result);
        return result;
      } catch (error) {
        // √âmettre l'√©v√©nement de completion (erreur)
        scrapingEventEmitter.emitScraperFailed(id, config.name, error);
        throw error;
      }
    };
  }

  // D√©terminer la ville en fonction de l'ID de mosqu√©e (version corrig√©e)
  getCityFromId(mosqueId) {
    // ‚úÖ CORRECTION: Mapping corrig√© selon la vraie r√©partition
    // Mosqu√©es de Walsall: IDs 1, 2, 3, 19, 20
    if ([1, 2, 3, 19, 20].includes(mosqueId)) {
      return "walsall";
    }
    // Mosqu√©es de Birmingham: IDs 4-18, 21-24
    else if (
      (mosqueId >= 4 && mosqueId <= 18) ||
      (mosqueId >= 21 && mosqueId <= 24)
    ) {
      return "birmingham";
    }

    // ‚úÖ Pour les nouvelles mosqu√©es (ID > 24), d√©tecter dynamiquement
    if (mosqueId > 24) {
      // R√©cup√©rer la ville depuis les m√©tadonn√©es du scraper
      const countryScrapersModule = getCountryScrapers();
      const allScrapers = countryScrapersModule.getAllScrapersFlat();
      const scraperData = allScrapers[mosqueId - 1]; // Approximation bas√©e sur l'ordre
      return scraperData ? scraperData.city.toLowerCase() : "unknown";
    }

    return "unknown";
  }

  // M√©thode pour obtenir le nom d'une mosqu√©e par ID (version dynamique)
  getMosqueName(mosqueId) {
    const countryScrapersModule = getCountryScrapers();
    const allScrapers = countryScrapersModule.getAllScrapersFlat();

    // ‚úÖ CORRECTION 2: Chercher dynamiquement au lieu d'un mapping statique
    // Approximation : l'ordre des scrapers correspond grosso modo aux IDs
    if (mosqueId <= allScrapers.length) {
      const scraperData = allScrapers[mosqueId - 1];
      return scraperData ? scraperData.name : `Mosqu√©e ID ${mosqueId}`;
    }

    return `Mosqu√©e ID ${mosqueId}`;
  }

  // Obtenir la taille de lot optimale selon la charge syst√®me
  getOptimalBatchSize() {
    const os = require("os");
    const freeMemory = os.freemem();
    const totalMemory = os.totalmem();
    const memoryUsage = (totalMemory - freeMemory) / totalMemory;

    if (memoryUsage < 0.5) return 12; // Syst√®me libre
    if (memoryUsage < 0.7) return 8; // Charge normale
    return 4; // Syst√®me charg√©
  }

  async runScraper(mosqueId) {
    logger.debug(
      `Tentative d'ex√©cution du scraper pour la mosqu√©e ID ${mosqueId}`
    );

    // D√©marrer le monitoring
    const execution = monitoring.startExecution(mosqueId);

    try {
      const scraper = this.scrapers.get(mosqueId);
      if (!scraper) {
        const error = new Error(
          `Aucun scraper trouv√© pour la mosqu√©e ID ${mosqueId}`
        );
        monitoring.recordFailure(execution, error);
        logger.error(error.message);
        throw error;
      }

      // Ex√©cuter le scraper
      const result = await scraper();

      // Enregistrer le succ√®s
      monitoring.recordSuccess(execution, result);

      return result;
    } catch (error) {
      // Enregistrer l'√©chec
      monitoring.recordFailure(execution, error);
      throw error;
    }
  }

  // ‚úÖ CORRECTION 1: Version CORRIG√âE qui attend TOUS les scrapers
  async runAllScrapers() {
    const startTime = Date.now();
    const results = [];
    const errors = [];
    const batchSize = this.getOptimalBatchSize();

    // R√©cup√©rer tous les IDs de scrapers configur√©s
    const scraperIds = Array.from(this.scrapers.keys());

    // Variable pour traquer les scrapers d√©j√† ex√©cut√©s dans cette session
    const executedScrapers = new Set();
    let actualMosquesToProcess = 0;

    // √âmettre l'√©v√©nement de d√©marrage du scraping global
    scrapingEventEmitter.emitGlobalScrapingStarted(scraperIds.length);

    // Log global au d√©but seulement
    logger.performance.globalComplete(scraperIds.length, 0, 0, 0); // Log de d√©marrage
    logger.info(
      `üöÄ D√©marrage scraping optimis√©: ${scraperIds.length} mosqu√©es (lots de ${batchSize})`
    );

    // ‚úÖ CORRECTION 1: Collecter TOUTES les promesses sans les attendre individuellement
    const allScraperPromises = [];

    // Traiter par lots adaptatifs
    const totalBatches = Math.ceil(scraperIds.length / batchSize);

    for (let i = 0; i < scraperIds.length; i += batchSize) {
      const batch = scraperIds.slice(i, i + batchSize);
      const batchNumber = Math.floor(i / batchSize) + 1;

      // V√©rifier quels scrapers peuvent r√©ellement √™tre ex√©cut√©s
      const scrapersToRun = batch.filter((id) => {
        if (executedScrapers.has(id)) return false;
        if (scraperQueue.isProcessing(id)) return false;
        if (scraperQueue.hasRecentExecution(id)) return false;

        executedScrapers.add(id);
        return true;
      });

      actualMosquesToProcess += scrapersToRun.length;

      if (scrapersToRun.length > 0) {
        logger.performance.batchStart(
          batchNumber,
          totalBatches,
          scrapersToRun.length
        );

        // ‚úÖ CORRECTION 1: Ajouter les promesses √† la liste globale au lieu de les attendre
        const batchPromises = scrapersToRun.map((id) =>
          this.runScraper(id)
            .then((result) => {
              if (result) {
                results.push(result);
                return { type: "success", result, mosqueId: id };
              }
            })
            .catch((error) => {
              const errorObj = {
                mosqueId: id,
                mosqueName: this.getMosqueName(id),
                error: error.message,
              };
              errors.push(errorObj);
              return { type: "error", error: errorObj, mosqueId: id };
            })
        );

        // Ajouter les promesses du lot √† la liste globale
        allScraperPromises.push(...batchPromises);

        // Log imm√©diat du lot (sans attendre les r√©sultats)
        logger.performance.batchComplete(
          batchNumber,
          scrapersToRun.length,
          0,
          0
        );
      }

      // Pause courte entre chaque lot
      if (i + batchSize < scraperIds.length) {
        await new Promise((resolve) => setTimeout(resolve, 500));
      }
    }

    // ‚úÖ CORRECTION 1: MAINTENANT attendre que TOUS les scrapers soient termin√©s
    logger.info(
      `‚è≥ Attente de la fin de tous les ${allScraperPromises.length} scrapers...`
    );

    await Promise.allSettled(allScraperPromises);

    // ‚úÖ CORRECTION CRITIQUE: Attendre que la queue soit r√©ellement vide
    logger.info(`‚è≥ Attente que la queue soit compl√®tement vide...`);
    await scraperQueue.waitForAll();

    // ‚úÖ CORRECTION CRITIQUE: Attendre un d√©lai suppl√©mentaire pour les scrapers asynchrones
    logger.info(`‚è≥ D√©lai de s√©curit√© pour les scrapers asynchrones...`);
    await new Promise((resolve) => setTimeout(resolve, 3000));

    // ‚úÖ CORRECTION CRITIQUE: V√©rifier qu'aucun scraper n'est encore actif
    let maxWaitAttempts = 10;
    while (scraperQueue.getStatus().active > 0 && maxWaitAttempts > 0) {
      logger.info(
        `‚è≥ Attente de ${scraperQueue.getStatus().active} scrapers restants...`
      );
      await new Promise((resolve) => setTimeout(resolve, 2000));
      maxWaitAttempts--;
    }

    if (scraperQueue.getStatus().active > 0) {
      logger.warn(
        `‚ö†Ô∏è ${
          scraperQueue.getStatus().active
        } scrapers encore actifs apr√®s attente maximale`
      );
    }

    // ‚úÖ NOUVELLE CORRECTION: Vider le cache API avant d'√©mettre l'√©v√©nement
    console.log(
      "üßπ Vidage du cache API pour forcer le rechargement des nouvelles donn√©es..."
    );
    try {
      // Vider le cache des requ√™tes pendantes pour forcer la r√©cup√©ration des nouvelles donn√©es
      if (
        global.mosqueTimesController &&
        global.mosqueTimesController.pendingRequests
      ) {
        global.mosqueTimesController.pendingRequests.clear();
        console.log("‚úÖ Cache API vid√© avec succ√®s");
      }
    } catch (error) {
      console.warn("‚ö†Ô∏è Impossible de vider le cache API:", error.message);
    }

    // √âmettre l'√©v√©nement de completion du scraping global
    scrapingEventEmitter.emitGlobalScrapingCompleted(results, errors);

    // ‚úÖ CORRECTION 1: Maintenant g√©n√©rer le r√©sum√© final (apr√®s que tout soit termin√©)
    const endTime = Date.now();
    const executionTime = endTime - startTime;
    const skippedMosques = scraperIds.length - actualMosquesToProcess;

    logger.performance.globalComplete(
      actualMosquesToProcess,
      results.length,
      errors.length,
      executionTime
    );

    if (skippedMosques > 0) {
      logger.info(
        `‚ÑπÔ∏è  ${skippedMosques} mosqu√©e(s) ignor√©e(s) (cache r√©cent ou en cours de traitement)`
      );
    }

    // Grouper par ville pour les logs d√©taill√©s
    const resultsByCity = {};
    const errorsByCity = {};

    results.forEach((result) => {
      const city = this.getCityFromId(result.mosqueId);
      const cityName =
        city === "walsall"
          ? "Walsall"
          : city === "birmingham"
          ? "Birmingham"
          : city.charAt(0).toUpperCase() + city.slice(1); // ‚úÖ Support dynamique

      if (!resultsByCity[cityName]) {
        resultsByCity[cityName] = [];
      }
      resultsByCity[cityName].push(result);
    });

    errors.forEach((error) => {
      const city = this.getCityFromId(error.mosqueId);
      const cityName =
        city === "walsall"
          ? "Walsall"
          : city === "birmingham"
          ? "Birmingham"
          : city.charAt(0).toUpperCase() + city.slice(1); // ‚úÖ Support dynamique

      if (!errorsByCity[cityName]) {
        errorsByCity[cityName] = [];
      }
      errorsByCity[cityName].push(error);
    });

    // Log d√©taill√© par ville
    const allCities = new Set([
      ...Object.keys(resultsByCity),
      ...Object.keys(errorsByCity),
    ]);
    allCities.forEach((city) => {
      const cityResults = resultsByCity[city] || [];
      const cityErrors = errorsByCity[city] || [];
      const totalForCity = cityResults.length + cityErrors.length;

      if (totalForCity > 0) {
        logger.info(
          `‚úÖ ${city}: ${cityResults.length} succ√®s, ${cityErrors.length} erreurs sur ${totalForCity} mosqu√©es`
        );
      }
    });

    // ‚úÖ CORRECTION 1: G√©n√©rer le rapport APR√àS que tout soit termin√©
    const monitoringReport = monitoring.generateRunAllScrapersReport({
      results,
      errors,
    });

    // Log des statistiques finales du pool
    scraperQueue.logPerformanceStats();

    return {
      results,
      errors,
      executionTime,
      resultsByCity,
      errorsByCity,
      completed: true,
      monitoringReport,
      optimizations: {
        batchSize: batchSize,
        totalBatches: totalBatches,
        poolOptimized: true,
        dynamicMapping: true, // ‚úÖ Nouvelle fonctionnalit√©
      },
    };
  }
}

// Instance par d√©faut pour la r√©trocompatibilit√©
const defaultScraperManager = new ScraperManager();

// Export complet pour la r√©trocompatibilit√© et la flexibilit√©
module.exports = {
  ScraperManager,
  // Getter dynamique pour exposer les scrapers
  get scrapers() {
    const scrapersObject = {};
    for (const [id, scraper] of defaultScraperManager.scrapers.entries()) {
      scrapersObject[id] = scraper;
    }
    return scrapersObject;
  },
  // Instance par d√©faut - r√©trocompatibilit√© avec l'ancien syst√®me
  runScraper: defaultScraperManager.runScraper.bind(defaultScraperManager),
  runAllScrapers: defaultScraperManager.runAllScrapers.bind(
    defaultScraperManager
  ),
  // Export aussi l'instance directement pour ceux qui en ont besoin
  default: defaultScraperManager,
  // Propri√©t√©s sp√©cifiques n√©cessaires pour la r√©trocompatibilit√©
  retryAttempts: defaultScraperManager.retryAttempts,
  MAX_RETRIES: defaultScraperManager.MAX_RETRIES,
  activeScrapings: defaultScraperManager.activeScrapings,
};
